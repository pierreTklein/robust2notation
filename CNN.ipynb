{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=7, micro=0, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bbb4ed0987f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\surface\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\surface\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\surface\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\surface\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\surface\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LAB_PATH = \"./data/train_labels.csv\"\n",
    "PREPROCESSED_TRAINING = \"./data/processedData.npy\"\n",
    "PREPROCESSED_KAGGLE = \"./data/processed_kaggle.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = ['apple', 'empty', 'moustache', 'mouth', 'mug', 'nail', 'nose', 'octagon', 'paintbrush', 'panda', 'parrot', 'peanut', 'pear', 'pencil', 'penguin', 'pillow', 'pineapple', 'pool', 'rabbit', 'rhinoceros', 'rifle', 'rollerskates', 'sailboat', 'scorpion', 'screwdriver', 'shovel', 'sink', 'skateboard', 'skull', 'spoon', 'squiggle']\n",
    "\n",
    "def getIndexOf(category):\n",
    "    return CATEGORIES.index(category)\n",
    "\n",
    "def getCategoryOf(index):\n",
    "    return CATEGORIES[index]\n",
    "\n",
    "def load(infile):\n",
    "    unformatted_images = np.load(infile, encoding='bytes')\n",
    "    formatted = []\n",
    "    for i,img in enumerate(unformatted_images):\n",
    "        formatted.append([i, img[0]])\n",
    "    return formatted\n",
    "\n",
    "def formatXData(X, xDimension = 40):\n",
    "    X = np.asarray(X)\n",
    "    # Convert to matrix form\n",
    "    X = X.reshape(-1, xDimension, xDimension, 1)\n",
    "    # Convert to float\n",
    "    X = X.astype('float32')\n",
    "    # Scale pixel values between 0 and 1\n",
    "    X = X / 255\n",
    "    return X.astype('float32')\n",
    "\n",
    "def addRotations(X,y):\n",
    "    newX = []\n",
    "    newY = []\n",
    "    for i,XMatrix in enumerate(X):\n",
    "        newX.append(XMatrix)\n",
    "        newY.append(y[i])\n",
    "        newX.append(np.rot90(XMatrix, 1))\n",
    "        newY.append(y[i])\n",
    "        newX.append(np.rot90(XMatrix, 2))\n",
    "        newY.append(y[i])\n",
    "        newX.append(np.rot90(XMatrix, 3))\n",
    "        newY.append(y[i])\n",
    "    return np.asarray(newX),np.asarray(newY)\n",
    "\n",
    "from scipy.ndimage import rotate, zoom\n",
    "\n",
    "# Centers the image\n",
    "def center(img):\n",
    "    minCoord, maxCoord = boundingBox(img)\n",
    "    xLength = maxCoord[0] - minCoord[0]\n",
    "    yLength = maxCoord[1] - minCoord[1]\n",
    "    newImg = [[0 for j in range(len(img[i]))] for i in range(len(img))]\n",
    "    \n",
    "    startX = int((len(img) - xLength) / 2)\n",
    "    startY = int((len(img[0]) - yLength) / 2)\n",
    "    for i,x in enumerate(range(startX, startX + xLength + 1)):\n",
    "        for j,y in enumerate(range(startY, startY + yLength + 1)):\n",
    "            newImg[x][y] = img[minCoord[0]+i][minCoord[1]+j]\n",
    "    return newImg\n",
    "\n",
    "# Crop out all of the white space. If you want square dimensions, then it will pad white space.\n",
    "def cropWhite(img, isSquare = False, whiteBoundary = True):\n",
    "    minCoord, maxCoord = boundingBox(img)\n",
    "    xLength = maxCoord[0] - minCoord[0] + 3\n",
    "    yLength = maxCoord[1] - minCoord[1] + 3\n",
    "    if isSquare:\n",
    "        xLength = max(xLength, yLength)\n",
    "        yLength = max(xLength, yLength)\n",
    "    \n",
    "    newImg = [[0 for j in range(yLength + 1)] for i in range(xLength + 1)]\n",
    "    for i in range(xLength):\n",
    "        for j in range(yLength):\n",
    "            # Check for case where we are out of bounds for cropped white + square\n",
    "            if (minCoord[0] + i) >= len(img) or (minCoord[1] + j) >= len(img[i]):\n",
    "                newImg[i + 1][j + 1] = 0\n",
    "            else:\n",
    "                newImg[i + 1][j + 1] = img[minCoord[0] + i][minCoord[1] + j]\n",
    "    return newImg\n",
    "\n",
    "# rescale image to square of height, width = dimension    \n",
    "def rescale(img, dimension, order = 0):\n",
    "    cropped = cropWhite(img)\n",
    "    height = len(cropped)\n",
    "    width = len(cropped[0])\n",
    "    zoomFactor = dimension / max(height, width)\n",
    "    return zoom(img, zoomFactor, order=order)\n",
    "\n",
    "def getRotations(x, y, interval_deg=30):\n",
    "    newX = []\n",
    "    newY = []\n",
    "    newX.append(x)\n",
    "    newY.append(y)\n",
    "    deg = interval_deg\n",
    "    while deg < 360:\n",
    "#         newX.append(np.rot90(XMatrix, 1))\n",
    "        newX.append(rotate(deg))\n",
    "        newY.append(y)\n",
    "        deg += interval_deg\n",
    "    return np.asarray(newX),np.asarray(newY)\n",
    "\n",
    "def preprocessRotImgs(x, y, rescaleDimension = 40, order = 1, interval_deg=30):\n",
    "    rotX, Y = getRotations(x, y)\n",
    "    for i in range(0, len(rotX)):\n",
    "        croppedImg = cropWhite(rotX[i], True)\n",
    "        centered_img = center(croppedImg)\n",
    "        rescaled_img = rescale(centered_img, rescaleDimension, order)\n",
    "        rotX[i] = rescaled_img\n",
    "    return rotX, Y\n",
    "\n",
    "def getRotData(X, Y, rescaleDimension = 40, interval_deg=30):\n",
    "    newX = []\n",
    "    newY = []\n",
    "    for i in range(0,len(X)):\n",
    "        newx, newy = preprocessRotImgs(X[i], Y[i])\n",
    "        newX.extend(newx)\n",
    "        newY.extend(newy)\n",
    "    return newX, newY\n",
    "\n",
    "def formatData(images, labels, xDimension = 40):\n",
    "    categories = list(set(labels['Category']))\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, img in enumerate(images):\n",
    "        label = labels.at[i,'Category']\n",
    "        categoryNum = getIndexOf(label)\n",
    "        X.append(img[1])\n",
    "        y.append(categoryNum)\n",
    "    y = to_categorical(y)\n",
    "    X = formatXData(X, xDimension)\n",
    "    return X.astype('float32'), y\n",
    "\n",
    "def split(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1) \n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imgs = load(PREPROCESSED_TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(TRAIN_LAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imgs, labels = getRotData(training_imgs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = formatData(training_imgs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y = addRotations(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 40\n",
    "num_classes = len(CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistory(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def createModel(input_shape=(40, 40, 1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=input_shape))\n",
    "\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def createModel2(input_shape=(40, 40, 1)):\n",
    "        # INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC\n",
    "    model = Sequential()\n",
    "    #Input\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     input_shape=input_shape))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, kernel_regularizer=keras.regularizers.l2(0.3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256, kernel_regularizer=keras.regularizers.l2(0.2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def leNet4():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Convolution\n",
    "    model.add(Conv2D(4, (4, 4), input_shape=(40, 40,1), activation='relu'))\n",
    "\n",
    "    # Subsampling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    # 2nd convolution layer\n",
    "    model.add( Conv2D(16, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Subsamplingmodel.add( Conv2D(64, (2, 2), activation='relu',\n",
    "\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(120, kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "#    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(128), )\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(40), )\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(31))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model1 = createModel()\n",
    "history = model1.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n",
    "score = model1.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_weights('./models/model1/model1_725.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_predictions = model1.predict(X_kaggle_2)\n",
    "ts = time.time()\n",
    "timestamp = int(ts)\n",
    "savePredictions('./models/model1/prediction_'+ str(timestamp) + '_' + str(score[1]) + '.csv', kaggle_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = createModel2()\n",
    "history = model2.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n",
    "plotHistory(history)\n",
    "score = model2.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_kaggle_2 = load(PREPROCESSED_KAGGLE)\n",
    "X_kaggle_2 = formatXData(list(map(lambda x: x[1], preprocessed_kaggle_2)))\n",
    "kaggle_predictions_2 = model2.predict(X_kaggle_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "timestamp = int(ts)\n",
    "savePredictions('./models/model2/prediction_'+ str(timestamp) + '_' + str(score[1]) + '.csv', kaggle_predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# With L2 regaularization\n",
    "model2L2Reg = createModel2()\n",
    "history = model2L2Reg.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n",
    "plotHistory(history)\n",
    "score = model2L2Reg.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2L2Reg.save_weights('./models/model2L2Reg/model2_6775.h5')\n",
    "model_json = model2L2Reg.to_json()\n",
    "with open(\"./models/model2L2Reg/model2_6775.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_predictions_2 = model2L2Reg.predict(X_kaggle_2)\n",
    "ts = time.time()\n",
    "timestamp = int(ts)\n",
    "savePredictions('./models/model2L2Reg/prediction_'+ str(timestamp) + '_' + str(score[1]) + '.csv', kaggle_predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model1.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "RFClassifier = RandomForestClassifier(n_estimators=94, criterion='gini')\n",
    "RFClassifier.fit(outputs, list(map(lambda y: np.argmax(y), y_val)))\n",
    "y_CNN_pred = model1.predict(X_test)\n",
    "y_RF_pred = RFClassifier.predict(y_CNN_pred)\n",
    "print(f1_score(list(map(lambda y: np.argmax(y), y_test)), y_RF_pred, average='micro'))\n",
    "#visualizePredictionsJustWrong(X, y_pred, y, 10, 30, (40, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = leNet4()\n",
    "history = lenet.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n",
    "plotHistory(history)\n",
    "score = lenet.evaluate(X_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = createModel2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_kaggle = load(PREPROCESSED_KAGGLE)\n",
    "# X_kaggle = formatXData(list(map(lambda x: x[1], preprocessed_kaggle)))\n",
    "# kaggle_predictions = model.predict(X_kaggle)\n",
    "\n",
    "def savePredictions(outfile, predictions):\n",
    "    with open(outfile,'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Category']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            index = np.argmax(prediction)\n",
    "            writer.writerow({'Id': i, 'Category': getCategoryOf(index)})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizePredictions(images, predictions, start = 0, end = 10, shape=(40, 40)):\n",
    "    fig, ax = plt.subplots(figsize=(len(CATEGORIES) + 1, 30))\n",
    "\n",
    "    for i in list(range(start, end)):\n",
    "\n",
    "        # plot probabilities:\n",
    "        ax = plt.subplot2grid((end - start, 5), (i - start, 0), colspan=4);\n",
    "        plt.bar(np.arange(len(CATEGORIES)), predictions[i], 0.35, align='center');\n",
    "        plt.xticks(np.arange(len(CATEGORIES)), CATEGORIES)\n",
    "        plt.tick_params(axis='x', bottom='off', top='off')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.ylim(0,1)\n",
    "        plt.subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # plot picture:\n",
    "        ax = plt.subplot2grid((end - start, 5), (i - start, 4));\n",
    "        plt.imshow(images[i].reshape(shape),cmap='gray_r', interpolation='nearest');\n",
    "        plt.xlabel(getCategoryOf(np.argmax(predictions[i]))); # get the label from the dict\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "#     for i, img in enumerate(images):\n",
    "#         img = img[1].reshape(shape).astype(np.uint8)\n",
    "#         f, axarr = plt.subplots(nrows=1, ncols=2, figsize=(20, 4))\n",
    "#         f.suptitle(getCategoryOf(np.argmax(predictions[i])))\n",
    "#         axarr[0].imshow(img, cmap='gray_r')\n",
    "#         axarr[0].set_title('Image')\n",
    "#         axarr[1].bar(range(0, len(CATEGORIES)), predictions[i])\n",
    "#         plt.show()\n",
    "def visualizePredictionsJustWrong(images, predictions, actual, start = 0, end = 10, shape=(40, 40)):\n",
    "    fig, ax = plt.subplots(figsize=(len(CATEGORIES) + 1, 30))\n",
    "    numFound = 0\n",
    "    for i in range(start, len(images)):\n",
    "        if getCategoryOf(np.argmax(predictions[i])) != getCategoryOf(np.argmax(actual[i])):\n",
    "            # plot probabilities:\n",
    "            ax = plt.subplot2grid((end - start, 5), (numFound, 0), colspan=4);\n",
    "            plt.bar(np.arange(len(CATEGORIES)), predictions[i], 0.35, align='center');\n",
    "            plt.xticks(np.arange(len(CATEGORIES)), CATEGORIES)\n",
    "            plt.tick_params(axis='x', bottom='off', top='off')\n",
    "            plt.ylabel('Probability')\n",
    "            plt.ylim(0,1)\n",
    "            plt.subplots_adjust(hspace = 0.5)\n",
    "\n",
    "            # plot picture:\n",
    "            ax = plt.subplot2grid((end - start, 5), (numFound, 4));\n",
    "            plt.imshow(images[i].reshape(shape),cmap='gray_r', interpolation='nearest');\n",
    "            plt.xlabel(getCategoryOf(np.argmax(predictions[i])) + \"/\" +getCategoryOf(np.argmax(actual[i]))); # get the label from the dict\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            \n",
    "            numFound += 1\n",
    "            if numFound >= end - start:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "timestamp = int(ts)\n",
    "savePredictions('./data/prediction_'+ str(timestamp) + '_' + str(score[1]) + '.csv', kaggle_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imgs = load(PREPROCESSED_TRAINING)\n",
    "X,y = formatData(training_imgs, labels)\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel()\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_kaggle_1 = load(PREPROCESSED_KAGGLE)\n",
    "X_kaggle_1 = formatXData(list(map(lambda x: x[1], preprocessed_kaggle_1)))\n",
    "kaggle_predictions_1 = model.predict(X_kaggle_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizePredictions(preprocessed_kaggle_1,kaggle_predictions_1, 10, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "timestamp = int(ts)\n",
    "savePredictions('./data/prediction_'+ str(timestamp) + '_' + str(score[1]) + '.csv', kaggle_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated AND bounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_TRAINING_BI = \"./data/preprocessed_interpolated_bounded.npy\"\n",
    "PREPROCESSED_KAGGLE_BI = \"./data/processed_kaggle_interpolated_bounded.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imgs_bi = load(PREPROCESSED_TRAINING_BI)\n",
    "X,y = formatData(training_imgs_bi, labels)\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = split(X,y)\n",
    "model_bi = createModel()\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n",
    "score = model_bi.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_bi.evaluate(X_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_kaggle_bi = load(PREPROCESSED_KAGGLE_BI)\n",
    "X_kaggle_bi = formatXData(list(map(lambda x: x[1], preprocessed_kaggle_bi)))\n",
    "kaggle_predictions_bi = model_bi.predict(X_kaggle_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "timestamp = int(ts)\n",
    "savePredictions('./data/prediction_'+ str(timestamp) + '_' + str(score[1]) + '.csv', kaggle_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizePredictions(preprocessed_kaggle_bi,kaggle_predictions_bi, 10, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel2()\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_predictions = model.predict(X_kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "timestamp = int(ts)\n",
    "savePredictions('./data/prediction_'+ str(timestamp) + '_' + str(score[1]) + '.csv', kaggle_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizePredictions(preprocessed_kaggle[:100],kaggle_predictions[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbed from: https://github.com/kradolfer/quickdraw-image-recognition/blob/master/quickdraw_image_recognition.ipynb\n",
    "def cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(42, (5, 5), input_shape=(40, 40, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = cnn_model()\n",
    "# Fit the model\n",
    "model_cnn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "scores = model_cnn.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_TRAINING_SMALL = \"./data/preprocessed_interpolated_28_28.npy\"\n",
    "PREPROCESSED_KAGGLE_SMALL = \"./data/preprocessed_kaggle_interpolated_28_28.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imgs_small = load(PREPROCESSED_TRAINING_SMALL)\n",
    "X_small,y_small = formatData(training_imgs_small, labels, 28)\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = split(X_small,y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi = createModel()\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val))\n",
    "score = model_bi.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding Box tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_TRAINING_BOUNDING_BOX = \"./data/train_preprocessed_boundingbox.npy\"\n",
    "PREPROCESSED_KAGGLE_BOUNDING_BOX = \"./data/kaggle_preprocessed_boundingbox.npy\"\n",
    "training_imgs_boundingbox = load(PREPROCESSED_TRAINING_BOUNDING_BOX)\n",
    "X_boundingbox,y_boundingbox = formatData(training_imgs_boundingbox, labels)\n",
    "X_train_bb, y_train_bb, X_test_bb, y_test_bb, X_val_bb, y_val_bb = split(X_boundingbox,y_boundingbox)\n",
    "model1_bb = createModel()\n",
    "history = model1.fit(X_train_bb, y_train_bb,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val_bb, y_val_bb))\n",
    "score = model1_bb.evaluate(X_test_bb, y_test_bb, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
