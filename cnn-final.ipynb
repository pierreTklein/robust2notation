{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "submissionCNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "HqDjZcRgrkl2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.version\n",
        "sys.version_info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o4Nr6Vy0rqqt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, ZeroPadding2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from keras import regularizers\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "scsxzFU7rsWB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ZIPPED_ORIGINAL = \"./data/processedData.zip\" #\"/content/drive/My Drive/processedData.zip\"\n",
        "ZIPPED_DATA = \"./data/extendedData.zip\" #\"/content/drive/My Drive/extendedData.zip\"\n",
        "ZIPPED_KAGGLE = \"./data/processed_kaggle.zip\" #\"/content/drive/My Drive/processed_kaggle.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilK4KoFiruHg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(ZIPPED_DATA,\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./data\")\n",
        "with zipfile.ZipFile(ZIPPED_KAGGLE, \"r\") as kagzip:\n",
        "    kagzip.extractall(\"./data\")\n",
        "with zipfile.ZipFile(ZIPPED_ORIGINAL, \"r\") as origzip:\n",
        "    origzip.extractall(\"./data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NkgnWgxvr02B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAIN_LAB_PATH = \"./data/extendedTrainLabels.csv\" # \"./data/train_labels.csv\"\n",
        "PREPROCESSED_TRAINING = \"./data/extendedTrainData.npy\" # \"./data/processedData.npy\"\n",
        "PREPROCESSED_KAGGLE = \"./data/processed_kaggle.npy\"\n",
        "PREPROCESSED_ORIGINAL = \"./data/processedData.npy\"\n",
        "ORIG_LAB_PATH = \"./data/train_labels.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmgMHSkQuEUd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CATEGORIES = ['apple', 'empty', 'moustache', 'mouth', 'mug', 'nail', 'nose', 'octagon', 'paintbrush', 'panda', 'parrot', 'peanut', 'pear', 'pencil', 'penguin', 'pillow', 'pineapple', 'pool', 'rabbit', 'rhinoceros', 'rifle', 'rollerskates', 'sailboat', 'scorpion', 'screwdriver', 'shovel', 'sink', 'skateboard', 'skull', 'spoon', 'squiggle']\n",
        "\n",
        "def getIndexOf(category):\n",
        "    return CATEGORIES.index(category)\n",
        "\n",
        "def getCategoryOf(index):\n",
        "    return CATEGORIES[index]\n",
        "\n",
        "def load(infile):\n",
        "    unformatted_images = np.load(infile, encoding='bytes')\n",
        "    formatted = []\n",
        "    for i,img in enumerate(unformatted_images):\n",
        "        formatted.append([i, img[0]])\n",
        "    return formatted\n",
        "\n",
        "def formatXData(X, xDimension = 40):\n",
        "    X = np.asarray(X)\n",
        "    # Convert to matrix form\n",
        "    X = X.reshape(-1, xDimension, xDimension, 1)\n",
        "    # Convert to float\n",
        "    X = X.astype('float32')\n",
        "    # Scale pixel values between 0 and 1\n",
        "    X = X / 255\n",
        "    return X.astype('float32')\n",
        "\n",
        "def addRotations(X,y):\n",
        "    newX = []\n",
        "    newY = []\n",
        "    for i,XMatrix in enumerate(X):\n",
        "        newX.append(XMatrix)\n",
        "        newY.append(y[i])\n",
        "        newX.append(np.rot90(XMatrix, 1))\n",
        "        newY.append(y[i])\n",
        "        newX.append(np.rot90(XMatrix, 2))\n",
        "        newY.append(y[i])\n",
        "        newX.append(np.rot90(XMatrix, 3))\n",
        "        newY.append(y[i])\n",
        "    return np.asarray(newX),np.asarray(newY)\n",
        "\n",
        "def formatData(images, labels, xDimension = 40, onehot=False):\n",
        "    if not onehot:\n",
        "        categories = list(set(labels['Category']))\n",
        "        X = []\n",
        "        y = []\n",
        "        for i, img in enumerate(images):\n",
        "            label = labels.at[i,'Category']\n",
        "            categoryNum = getIndexOf(label)\n",
        "            X.append(img[1])\n",
        "            y.append(categoryNum)\n",
        "        y = to_categorical(y)\n",
        "        X = formatXData(X, xDimension)\n",
        "    else:\n",
        "        X = []\n",
        "        y = labels\n",
        "        for i, img in enumerate(images):\n",
        "            X.append(img[1])\n",
        "        X = formatXData(X, xDimension)\n",
        "    return X.astype('float32'), y\n",
        "\n",
        "def split(X,y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1) \n",
        "    return X_train, y_train, X_test, y_test, X_val, y_val\n",
        "\n",
        "def visualizePredictions(images, predictions, indices, shape=(40, 40)):\n",
        "    fig, ax = plt.subplots(figsize=(len(CATEGORIES) + 1, len(indices) * 3))\n",
        "\n",
        "    for spot, i in enumerate(indices):\n",
        "\n",
        "        # plot probabilities:\n",
        "        ax = plt.subplot2grid((len(indices), 5), (spot, 0), colspan=4);\n",
        "        plt.bar(np.arange(len(CATEGORIES)), predictions[i], 0.35, align='center');\n",
        "        plt.xticks(np.arange(len(CATEGORIES)), CATEGORIES)\n",
        "        plt.tick_params(axis='x', bottom='off', top='off')\n",
        "        plt.ylabel('Probability')\n",
        "        plt.ylim(0,1)\n",
        "        plt.subplots_adjust(hspace = 0.5)\n",
        "\n",
        "        # plot picture:\n",
        "        ax = plt.subplot2grid((len(indices), 5), (spot, 4));\n",
        "        plt.imshow(images[i].reshape(shape),cmap='gray_r', interpolation='nearest');\n",
        "        plt.xlabel(getCategoryOf(np.argmax(predictions[i]))); # get the label from the dict\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "def visualizePredictionsJustWrong(images, predictions, actual, start = 0, end = 10, shape=(40, 40)):\n",
        "    fig, ax = plt.subplots(figsize=(len(CATEGORIES) + 1, 30))\n",
        "    numFound = 0\n",
        "    for i in range(start, len(images)):\n",
        "        if getCategoryOf(np.argmax(predictions[i])) != getCategoryOf(np.argmax(actual[i])):\n",
        "            # plot probabilities:\n",
        "            ax = plt.subplot2grid((end - start, 5), (numFound, 0), colspan=4);\n",
        "            plt.bar(np.arange(len(CATEGORIES)), predictions[i], 0.35, align='center');\n",
        "            plt.xticks(np.arange(len(CATEGORIES)), CATEGORIES)\n",
        "            plt.tick_params(axis='x', bottom='off', top='off')\n",
        "            plt.ylabel('Probability')\n",
        "            plt.ylim(0,1)\n",
        "            plt.subplots_adjust(hspace = 0.5)\n",
        "\n",
        "            # plot picture:\n",
        "            ax = plt.subplot2grid((end - start, 5), (numFound, 4));\n",
        "            plt.imshow(images[i].reshape(shape),cmap='gray_r', interpolation='nearest');\n",
        "            plt.xlabel(str(i) + \":\" +getCategoryOf(np.argmax(predictions[i])) + \"/\" +getCategoryOf(np.argmax(actual[i]))); # get the label from the dict\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            \n",
        "            numFound += 1\n",
        "            if numFound >= end - start:\n",
        "                return\n",
        "            \n",
        "def plotHistory(history):\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "def loadModel(path):\n",
        "    return load_model(path)\n",
        "\n",
        "def savePredictions(outfile, predictions):\n",
        "    if outfile == '':\n",
        "        for i, prediction in enumerate(predictions):\n",
        "            index = np.argmax(prediction)\n",
        "            print(i,getCategoryOf(index))\n",
        "    else:\n",
        "        with open(outfile,'w') as csvfile:\n",
        "            fieldnames = ['Id', 'Category']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for i, prediction in enumerate(predictions):\n",
        "                index = np.argmax(prediction)\n",
        "                writer.writerow({'Id': i, 'Category': getCategoryOf(index)})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qaW8fo7c94Hh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def paddingAndNormalization(input_shape=(40, 40, 1)):\n",
        "    model = Sequential()\n",
        "    model.add(ZeroPadding2D(padding=(1, 1), input_shape=input_shape))\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 kernel_initializer='he_normal'))\n",
        "\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "#     model.add(Dropout(0.25))\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ZeroPadding2D(padding=(1, 1)))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mdU9P349uKzt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 40\n",
        "num_classes = len(CATEGORIES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1eI0Oecuedn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Separate originals from transformed images for sanity\n",
        "training_imgs = load(PREPROCESSED_TRAINING)\n",
        "labels = np.genfromtxt(TRAIN_LAB_PATH, skip_header=0)\n",
        "origs_img = []\n",
        "origs_lab = []\n",
        "labels = labels.tolist()\n",
        "idx = int(len(training_imgs)/60)\n",
        "\n",
        "for i in range(0, idx):\n",
        "    j = 60*i - i\n",
        "    origs_img.append(training_imgs.pop(j))\n",
        "    origs_lab.append(labels.pop(j))\n",
        "    \n",
        "labels = np.array(labels)\n",
        "origs_lab = np.array(origs_lab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ETzyPZYPu7g9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X, y are extended data minus originals. X_val, y_val are the originals.\n",
        "X,y = formatData(training_imgs, labels, onehot=True) # Onehot because comes from concurrent_rotate.py\n",
        "X_val, y_val = formatData(origs_img, origs_lab, onehot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lMBTVE0xu8PA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Colab GPU memory!\n",
        "del training_imgs\n",
        "del labels\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19ekYPzq-xWK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# New model\n",
        "model5 = paddingAndNormalization()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iq2OMdJI_UpJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Keep theses from history across training!\n",
        "losses = []\n",
        "accs = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SPq6ksXWvJDk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initial 40 epochs on extended data.\n",
        "history = model5.fit(X, y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=40,\n",
        "          verbose=1,\n",
        "          validation_data=(X_val, y_val))\n",
        "\n",
        "losses.extend(history.history['loss'])\n",
        "accs.extend(history.history['acc'])\n",
        "\n",
        "model5.save('model.h5')\n",
        "np.savetxt('modelacc.txt', accs)\n",
        "np.savetxt('modelloss.txt', losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7NIkqVCy_IxJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# If continuing training from saved trained model from above, start here, else ignore.\n",
        "model5 = load_model('model.h5')\n",
        "accs = np.loadtxt('modelacc.txt').tolist()\n",
        "losses = np.loadtxt('modelloss.txt').tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myr5CnBmeg7P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# If continuing from saved extra-trained model from below, start here, else ignore.\n",
        "model5 = load_model('model-extra.h5')\n",
        "# Load saved loss/acc history for extra-trained model.\n",
        "losses = np.loadtxt('modelacc-extra.txt').tolist()\n",
        "accs = np.loadtxt('modelloss-extra.txt').tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ug576zjm_JvC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 5 epochs extended data / 10 epochs original data (for submission to Kaggle, so train on all data).\n",
        "history = model5.fit(X, y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=5,\n",
        "          verbose=1)#,\n",
        "#           validation_data=(X_val, y_val))\n",
        "\n",
        "losses.extend(history.history['loss'])\n",
        "accs.extend(history.history['acc'])\n",
        "\n",
        "history = model5.fit(X_val, y_val,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          verbose=1)\n",
        "\n",
        "losses.extend(history.history['loss'])\n",
        "accs.extend(history.history['acc'])\n",
        "\n",
        "preprocessed_kaggle = load(PREPROCESSED_KAGGLE)\n",
        "X_kaggle = formatXData(list(map(lambda x: x[1], preprocessed_kaggle)))\n",
        "kaggle_predictions = model5.predict(X_kaggle)\n",
        "\n",
        "# Save predictions\n",
        "savePredictions('model-extra.csv',kaggle_predictions)\n",
        "# Save model with extra training\n",
        "model5.save('model-extra.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "# Save loss/accuracy history\n",
        "np.savetxt('modelacc-extra.txt', accs)\n",
        "np.savetxt('modelloss-extra.txt', losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M_bto1SrAcFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Summarize history for accs\n",
        "plt.plot(accs)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(losses)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}